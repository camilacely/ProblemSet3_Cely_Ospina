my_html <-
'<!DOCTYPE html>
<html>
<meta charset="utf-8">
<head>
<title> Título de la página: ejemplo de clase </title>
</head>
<body>
<h1> Title 1.</h1>
<h2> Subtitle <u>subrayado-1</u>. </h2>
<p> Este es un párrafo muy pequeño que se encuentra dentro de la etiqueta <b>p</b> de <i>html</i> </p>
</body>
</html>'
write.table(x=my_html , file='my_page.html' , quote=F , col.names=F , row.names=F)
browseURL("my_page.html") ## leer con el navegador de su equipo
my_html <-
'<!DOCTYPE html>
<html>
<meta charset="utf-8">
<head>
<title> Título de la página: ejemplo de clase </title>
</head>
<body>
<h1> Title 1 Camila Cely.</h1>
<h2> Subtitle <u>subrayado-1</u>. </h2>
<p> Este es un párrafo muy pequeño que se encuentra dentro de la etiqueta <b>p</b> de <i>html</i> </p>
</body>
</html>'
write.table(x=my_html , file='my_page.html' , quote=F , col.names=F , row.names=F)
browseURL("my_page.html") ## leer con el navegador de su equipo
vignette("rvest")
my_url = "https://es.wikipedia.org/wiki/Copa_Mundial_de_F%C3%BAtbol"
browseURL(my_url) ## Ir a la página
my_html = read_html(my_url) ## leer el html de la página
class(my_html) ## ver la clase del objeto
View(my_html)
## Obtener los elementos h2 de la página
my_html %>% html_elements("h2")
## Ver los textos
my_html %>% html_elements("h2") %>% html_text()
my_html %>% html_nodes(xpath = '//*[@id="mw-content-text"]/div/p[1]') #este es el de clase
my_html %>% html_nodes(xpath = '//*[@id="mw-content-text"]/div/p[1]')  %>%
html_text()
## extraer todas las tablas del html
my_table = my_html %>% html_table()
## numero de tablas extraidas
length(my_table)
my_table[[11]]
sub_html = my_html %>% html_nodes(xpath='//*[@id="mw-content-text"]/div[1]/table[10]/tbody')
class(sub_html)
elements = sub_html %>% html_nodes("a")
elements[1:5]
titles = elements %>% html_attr("title")
titles[1:5]
refs = elements %>% html_attr("href")
refs[1:5]
vignette("rvest")
rm(list=ls())
install.packages(pacman)
require(pacman)
install.packages(pacman)
p_load(tidyverse,rvest)
url1<-"https://ignaciosarmiento.github.io/GEIH2018_sample/pages/geih_page_1.html"
browseURL(url1)
url1<-"https://ignaciomsarmiento.github.io/GEIH2018_sample/page1.html"
browseURL(url1)
tabla1<-url1%>% html_table()
my_html1 = read_html(url1)
View(my_html1)
class(my_html)
class(my_html1)
view(my_html1)
view(my_html1)
view(my_html1)
my_table1<-my_html1 %>% html_table()
View(my_table1)
length(my_table1)
my_html1 %>% html_nodes
my_html1 %>% html_nodes(xpath = '/html/body/div/div/div[2]/div/table')
my_table1<-my_html1 %>% html_table()
rm(list=ls())
url1<-"https://ignaciomsarmiento.github.io/GEIH2018_sample/page1.html"
tabla1<- url1 %>% read_html() %>% html_table()
tabla1<- url1 %>% read_html(url1) %>% html_table()
tabla1<- url1 %>% read_html() %>% html_table()
require(rvest)
tabla1<- url1 %>% read_html() %>% html_table()
vignette("rvest")
my_html1 <- read_html(url1)
class(my_html1)
my_html1 %>%
html_element("body") %>%
html_text2() %>%
cat()
my_html1 %>%
html_element("table") %>%
html_text2() %>%
cat()
class(my_html1)
tabla1<- url1 %>% read_html() %>% html_table()
length(tabla1)
install.packages(bootstrap)
install.packages(bootstrap)
install.packages(boot)
install.packages(boot)
library(boot, lib.loc = "C:/Program Files/R/R-4.1.1/library")
library(boot)
install.packages(caret)
library(caret)
install.packages("caret")
library("class")
library("MASS")
data(fgl)
str(fgl)
nearest1<-knn(train=x[-test], cl=fgl$type[-test], k=1)
x<-fgl
nearest1<-knn(train=x[-test], cl=fgl$type[-test], k=1) #aqui le decimos que nos pruebe, segun el mas cercano, cual seria la probabilidad
set.seed(1010101)
test<-sample(1:214,10) #aqui hay 10 obs solo para efectos de la clase
nearest1<-knn(train=x[-test], cl=fgl$type[-test], k=1) #aqui le decimos que nos pruebe, segun el mas cercano, cual seria la probabilidad
test<-sample(1:214,10)
nearest1<-knn(train=x[-test], cl=fgl$type[-test], k=1) #aqui le decimos que nos pruebe, segun el mas cercano, cual seria la probabilidad
x<-fgl
set.seed(1010101)
test<-sample(1:214,10) #aqui hay 10 obs solo para efectos de la clase
nearest1<-knn(train=x[-test], cl=fgl$type[-test], k=1) #aqui le decimos que nos pruebe, segun el mas cercano,
#cual seria la probabilidad
nearest5<-knn(train=x[-test], cl=fgl$type[-test], k=5)
data.frame(fgl$type[test],nearest1,nearest5)
data(fgl)
str(fgl)
x<-fgl
set.seed(1010101)
test<-sample(1:214,10) #aqui hay 10 obs solo para efectos de la clase
nearest1<-knn(train=x[-test], cl=fgl$type[-test], k=1) #aqui le decimos que nos pruebe, segun el mas cercano,
#cual seria la probabilidad
nearest5<-knn(train=x[-test], cl=fgl$type[-test], k=5)
data.frame(fgl$type[test],nearest1,nearest5) #todo esto todavia no está corriendo porque no alcance a copiar la ppt anterior
#pero la idea es que cada nearest nos muestra, de entre los x mas c
library("dplyr")
library("gamlr")
credit<-readRDS("credit.class.rds")
dim(credit)
library("gamlr")
install.packages(gamlr)
install.packages(gamlr)
library("gamlr")
install.packages(gamlr)
install.packages("gamlr")
library("gamlr")
credit<-readRDS("credit.class.rds") #voy a anotar el codigo pero no lo he podido correr
dim(credit)
library("dplyr")
library("dplyr")
library("gamlr")
credit<-readRDS("credit.class.rds") #voy a anotar el codigo pero no lo he podido correr
dim(credit)
chi.poly<-read_sf ("C:/Users/Camila Cely/Documents/MECA/INTERSEMESTRALES/Big Data/foreclosures/foreclosures.shp")
require("sf")
require("spdep")
require("dplyr")
chi.poly<-read_sf ("C:/Users/Camila Cely/Documents/MECA/INTERSEMESTRALES/Big Data/foreclosures/foreclosures.shp")
require(pacman)
p_load(tidyverse,
sf,
spdep,
dplyr)
chi.poly<-read_sf ("C:/Users/Camila Cely/Documents/MECA/INTERSEMESTRALES/Big Data/foreclosures/foreclosures.shp")
plot(chi.poly[violent])
plot(chi.poly[violent])
list.queen<-poly2nb(chi.poly, queen=TRUE) #armar una lista que muestra por ejemplo para la region 1 quienes son sus vecinos #vecinos del tipo reina, si uno pone false le sale torre
W<-nb2listw(list.queen, style="W", zero.policy=TRUE)
W
coords<-st_centroid(st_geometry(chi.poly), of_largest_polygon=TRUE)
W_dist<-dnearneigh(coords,C,1000)
W_dist
st.crs(chi.poly)<-4326
chi.poly<-st_transform(chi.poly,26916)
st_crs(chi.poly)<-4326
chi.poly<-st_transform(chi.poly,26916
coords<-st_centroid(st_geometry(chi.poly), of_largest_polygon=TRUE)
W_dist<-dnearneigh(coords,C,1000)   #aqui esto no me corre PORQUE NO ANOTE LA LINEA EN LA QUE SE PONIA EN GEOMETRIA PLANAR, BUSCAR EN PPT
W_dist
chi.poly<-st_transform(chi.poly,26916) #este numero es para chicago, buscar para bogota y medellin
list.queen<-poly2nb(chi.poly, queen=TRUE) #armar una lista que muestra por ejemplo para la region 1 quienes son sus vecinos #vecinos del tipo reina, si uno pone false le sale torre
W<-nb2listw(list.queen, style="W", zero.policy=TRUE) #si uno pusiera nb2mat le arma la matriz pero eso es computacionalmente muy costoso y hace lo mismo
W
#Characteristics of weights list object:
#  Neighbour list object:
#  Number of regions: 897
#Number of nonzero links: 6140
#Percentage nonzero weights: 0.7631036
#Average number of links: 6.845039 #EN PROMEDIO CADA QUIEN TIENE CASI 7 VECINOS
#Weights style: W
#Weights constants summary:
#  n     nn  S0       S1       S2
#W 897 804609 897 274.4893 3640.864
coords<-st_centroid(st_geometry(chi.poly), of_largest_polygon=TRUE)
W_dist<-dnearneigh(coords,C,1000)   #aqui esto no me corre PORQUE NO ANOTE LA LINEA EN LA QUE SE PONIA EN GEOMETRIA PLANAR, BUSCAR EN PPT
W_dist
W_dist<-dnearneigh(coords,C,1000)
chi.ols<-lm(violent~ est_fcs_rt + bls_unemp, data= chi.poly) #aqui empezamos a probar un modelo
summary(chi.ols)
moran.lm<-lm.morantest(chi.ols,W, alternative="two.sided") #alternativa de dos colas, la que ignacio suele hacer
print(moran.lm)
chi.poly<-st_transform(chi.poly,26916)
ist.queen<-poly2nb(chi.poly, queen=TRUE) #armar una lista que muestra por ejemplo para la region 1 quienes son sus vecinos #vecinos del tipo reina, si uno pone false le sale torre
W<-nb2listw(list.queen, style="W", zero.policy=TRUE) #si uno pusiera nb2mat le arma la matriz pero eso es computacionalmente muy costoso y hace lo mismo
W
coords<-st_centroid(st_geometry(chi.poly), of_largest_polygon=TRUE)
W_dist<-dnearneigh(coords,C,1000)   #aqui esto no me corre PORQUE NO ANOTE LA LINEA EN LA QUE SE PONIA EN GEOMETRIA PLANAR, BUSCAR EN PPT
sar.chi<-lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
summary(sar.chi)
sar.chi<-lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
sar.chi<- lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
install.packages("spdep")
install.packages("spdep")
sar.chi<- lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
summary(sar.chi)
sar.chi<- lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
p_load(tidyverse,
sf,
spdep,
dplyr)
require(pacman)
p_load(tidyverse,
sf,
spdep,
dplyr)
sar.chi<- lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
summary(sar.chi)
require(spdep)
sar.chi<- lagsarlm(violent~ est_fcs_rt + bls_unemp, data= chi.poly, W)
## clean environment
rm(list=ls())
## Llamar/instalar las librerias
require(pacman)
p_load(tidyverse,    #Para limpiar los datos
caret,        #Para la clasificación y regresiones
rio,          #Para importar datos
modelsummary, # msummary
gamlr,
class,
ggplot2,
skimr,
rvest,
dplyr,
stargazer,
gtsummary,
expss,
fastAdaboost,
randomForest,
xgboost,
glmnet,
pROC) #por ahora llame todas las del problem set 2
predict<- stats::predict  #con esto soluciono el problema de que haya mas de una libreria con este comando
setwd("C:/Users/Camila Cely/Documents/GitHub/ProblemSet3_Cely_Ospina")
test<-readRDS("stores/test.Rds")
train<-readRDS("stores/train.Rds")
intersect(names(test), names(train))
View(train)
summary(train$ad_type)
summary(train$start_date)
summary(train$end_date)
summary(train$created_on)
summary(train$rooms)
summary(train$bedrooms)
summary(train$bathrooms)
sum(train$bathrooms == 20)
train <- train %>% drop_na(c("bathrooms"))
summary(train$surface_total)
summary(train$surface_covered)
hist(train$surface_total)
hist(train$surface_covered)
#"Prediction of housing values"
#####################
# 1. Data Acquisition
#####################
## clean environment
rm(list=ls())
## Llamar/instalar las librerias
require(pacman)
p_load(tidyverse,    #Para limpiar los datos
caret,        #Para la clasificación y regresiones
rio,          #Para importar datos
modelsummary, # msummary
gamlr,
class,
ggplot2,
skimr,
rvest,
dplyr,
stargazer,
gtsummary,
expss,
fastAdaboost,
randomForest,
xgboost,
glmnet,
pROC) #por ahora llame todas las del problem set 2
predict<- stats::predict  #con esto soluciono el problema de que haya mas de una libreria con este comando
#####################
##cargar los datos #
#####################
##Establecer el directorio
#setwd
#setwd("C:/Users/SARA/Documents/ESPECIALIZACIÓN/BIG DATA/GITHUB/ProblemSet2_Cely_Ospina")
setwd("C:/Users/Camila Cely/Documents/GitHub/ProblemSet3_Cely_Ospina")
#traer las bases de train y de test
test<-readRDS("stores/test.Rds")     #11.150 obs
train<-readRDS("stores/train.Rds")  #107.567 obs
#####################
##1. Limpieza de datos:
###################
#cuales son las variables que aparecen tanto en train como en test
intersect(names(test), names(train))
# [1] "property_id"     "ad_type"         "start_date"      "end_date"        "created_on"      "lat"
#[7] "lon"             "l1"              "l2"              "l3"              "rooms"           "bedrooms"
#[13] "bathrooms"       "surface_total"   "surface_covered" "currency"        "title"           "description"
#[19] "property_type"   "operation_type"
#vamos a ver en general como se comportan las variables
#fechas
summary(train$start_date) #estas fechas si dan bien
summary(train$end_date) #estas fechas, por encima de la mediana, tienen cosas raras, ejemplo año 4000 o 9000
#ademas no estoy segura de que quieren decir las fechas
summary(train$created_on) #estas dan bien
#coordenadas
var_lab(train$lat) = "Latitud"
var_lab(train$lon) = "Longitud"
#localizacion general
var_lab(train$l2) = "Departamento"
var_lab(train$l3) = "Municipio"
#cuartos
summary(train$rooms) #esta tiene 53.606 NAs, promedio= 2.98
summary(train$bedrooms) #sin NAs, promedio =3.08 #propongo usar esta porque tienen casi la misma informacion, mismo min y max
var_lab(train$bedrooms) = "Num de cuartos"
#baños
summary(train$bathrooms) # tiene 30.074 NAs
#   Min. 1st Qu.  Median   Mean   3rd Qu.  Max.    NA's
#  1.000   2.000   2.000   2.723   3.000  20.000   30074
#tiene este valor maximo de 20 bathrooms que creo que es outlier, podemos eliminarlo
#que hacer con los NAs?
#opcion 1= imputar que las viviendas tienen minimo un baño
#opcion 2= eliminar estas observaciones
#Por ahora creo que lo mejor es eliminar las observaciones porque, si bien las viviendas deben tener minimo un baño #REVISAR
#podrian tener dos e imputarles uno solo nos puede estar modificando mucho las predicciones
#intuicion = el numero de baños afecta mucho el precio de la vivienda
#train <- train %>% drop_na(c("bathrooms")) #aqui quedaria una base con 77.493 obs
#POR AHORA NO LA VOY A CORRER PORQUE NO ESTOY SEGURA DE LA DECISION DE ELIMINAR LOS NAs
var_lab(train$bathrooms) = "Num de banos"
#area
summary(train$surface_total) #NAs = 49.936 #estan saliendo demasiados NAs, no podemos eliminar todo esto o nos quedamos sin obs
#va a tocar imputarle valores promedio de acuerdo con otras caracteristicas
summary(train$surface_covered) #57.515 NAs
train <- train %>%
mutate(area_descubierta = (train$surface_total - train$surface_covered  ))
summary(train$area_descubierta)
view(train$area_descubierta)
summary(train$currency)
view(train$currency)
sum(train$currency == "USD")
sum(train$currency == "COP")
view(train$title)
view(train$title)
view(train$description)
view(train$property_type)
class(train$property_type)
train$property_type <- as.factor(train$property_type)
class(train$property_type)
view(train$operation_type)
sum(train$currency == "Venta")
sum(train$operation_type == "Venta")
train$property_type <- as.factor(train$l2)
class(train$l2)
###################################
## Big Data - Problem Set 3 #######
# Maria Camila Cely , Sara Ospina #
###### Julio 2022 #################
###################################
#"Prediction of housing values"
#####################
# 1. Data Acquisition
#####################
## clean environment
rm(list=ls())
## Llamar/instalar las librerias
require(pacman)
p_load(tidyverse,    #Para limpiar los datos
caret,        #Para la clasificación y regresiones
rio,          #Para importar datos
modelsummary, # msummary
gamlr,
class,
ggplot2,
skimr,
rvest,
dplyr,
stargazer,
gtsummary,
expss,
fastAdaboost,
randomForest,
xgboost,
glmnet,
pROC) #por ahora llame todas las del problem set 2
predict<- stats::predict  #con esto soluciono el problema de que haya mas de una libreria con este comando
#####################
##cargar los datos #
#####################
##Establecer el directorio
#setwd
#setwd("C:/Users/SARA/Documents/ESPECIALIZACIÓN/BIG DATA/GITHUB/ProblemSet2_Cely_Ospina")
setwd("C:/Users/Camila Cely/Documents/GitHub/ProblemSet3_Cely_Ospina")
#traer las bases de train y de test
test<-readRDS("stores/test.Rds")     #11.150 obs
train<-readRDS("stores/train.Rds")  #107.567 obs
#####################
##1. Limpieza de datos:
###################
#cuales son las variables que aparecen tanto en train como en test
intersect(names(test), names(train))
# [1] "property_id"     "ad_type"         "start_date"      "end_date"        "created_on"      "lat"
#[7] "lon"             "l1"              "l2"              "l3"              "rooms"           "bedrooms"
#[13] "bathrooms"       "surface_total"   "surface_covered" "currency"        "title"           "description"
#[19] "property_type"   "operation_type"
#vamos a ver en general como se comportan las variables
#fechas
summary(train$start_date) #estas fechas si dan bien
summary(train$end_date) #estas fechas, por encima de la mediana, tienen cosas raras, ejemplo año 4000 o 9000
#ademas no estoy segura de que quieren decir las fechas
summary(train$created_on) #estas dan bien
#coordenadas
var_lab(train$lat) = "Latitud"
var_lab(train$lon) = "Longitud"
#localizacion general
var_lab(train$l2) = "Departamento"
var_lab(train$l3) = "Municipio"
#cuartos
summary(train$rooms) #esta tiene 53.606 NAs, promedio= 2.98
summary(train$bedrooms) #sin NAs, promedio =3.08 #propongo usar esta porque tienen casi la misma informacion, mismo min y max
var_lab(train$bedrooms) = "Num de cuartos"
#baños
summary(train$bathrooms) # tiene 30.074 NAs
#   Min. 1st Qu.  Median   Mean   3rd Qu.  Max.    NA's
#  1.000   2.000   2.000   2.723   3.000  20.000   30074
#tiene este valor maximo de 20 bathrooms que creo que es outlier, podemos eliminarlo
#que hacer con los NAs?
#opcion 1= imputar que las viviendas tienen minimo un baño
#opcion 2= eliminar estas observaciones
#Por ahora creo que lo mejor es eliminar las observaciones porque, si bien las viviendas deben tener minimo un baño #REVISAR
#podrian tener dos e imputarles uno solo nos puede estar modificando mucho las predicciones
#intuicion = el numero de baños afecta mucho el precio de la vivienda
#train <- train %>% drop_na(c("bathrooms")) #aqui quedaria una base con 77.493 obs
#POR AHORA NO LA VOY A CORRER PORQUE NO ESTOY SEGURA DE LA DECISION DE ELIMINAR LOS NAs
var_lab(train$bathrooms) = "Num de banos"
#area
summary(train$surface_total) #NAs = 49.936 #estan saliendo demasiados NAs, no podemos eliminar todo esto o nos quedamos sin obs
#va a tocar imputarle valores promedio de acuerdo con otras caracteristicas
summary(train$surface_covered) #57.515 NAs
#    Min. 1st Qu.  Median   Mean  3rd Qu.  Max.    NA's  #surface_total
#    11      70     108     173     189   108800   49936
#   Min. 1st Qu.  Median   Mean  3rd Qu.   Max.    NA's   #surface_covered
#   1.0    73.0   110.0   148.3   188.0  11680.0   57515   #notar que no se comportan tan tan diferente
#por algun motivo no hay datos demasiado buenos de area, revisar
#podriamos crear una variable que relacione estas dos
#train <- train %>%
#  mutate(area_descubierta = (train$surface_total - train$surface_covered  ))
#summary(train$area_descubierta)
#view(train$area_descubierta)  #la verdad se generan 96.625 NAs y valores negativos #las voy a dejar para que no corran porque por ahora creo que esta variable no es necesaria
#creo que lo mejor sera guiarnos por surface_total
var_lab(train$surface_total) = "Area total"
#dinero
summary(train$currency) #aqui nos dice que estan medidas en COP
view(train$currency)
sum(train$currency == "USD") #sale 0  #vemos que no hay valores en dolares
sum(train$currency == "COP") #107567  #la totalidad de los precios estan en COP
#anuncio
view(train$title) #aqui sale el titulo de la oferta, ejemplo= "hermosa casa en venta"
var_lab(train$title) = "Titulo anuncio"
view(train$description) #aqui dice los detalles, ejemplo = barrio, "muy iluminado", "moderno", etc7
var_lab(train$description) = "Descripcion anuncio"
#tipo de propiedad
view(train$property_type) #esta nos dice si es casa o apartamento
var_lab(train$property_type) = "Tipo de propiedad"
#venta o arriendo
view(train$operation_type) #aqui nos dice si es venta o arriendo
sum(train$operation_type == "Venta") #107.567 #la totalidad de propiedades estan en venta
#Entonces = las variables que considero que mas nos importan son las siguientes
#train$start_date
#train$lat
#train$lon
#train$l2
#train$l3
#train$bedrooms
#train$bathrooms
#train$surface_total
#train$title
#train$description
#train$property_type
#VOLVER LAS VARIABLES QUE SE NECESITEN COMO AS.FACTOR
train$l2 <- as.factor(train$l2)
class(train$l2)
train$l3 <- as.factor(train$l3)
class(train$l3)
train$property_type <- as.factor(train$property_type)
class(train$property_type)
View(train)
summary(train$price)
hist(train$price)
train %>%
select(price, start_date, l2, bedrooms, bathrooms, surface_total, property_type) %>%
tbl_summary()
